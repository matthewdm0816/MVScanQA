# _Advancing 3D Scene Understanding with MV-ScanQA Multi-View Reasoning Evaluation and TripAlign Pre-training Dataset_ - Official Codebase

This work is accepeted by ACM MM 2025. [Project Page](matthewdm0816.github.io/mv-scanqa/)

## Installation
0. Create a new conda environment if necessary.
1. Install `torch` 1.12 following [PyTorch official website](https://pytorch.org/get-started/locally/)
2. Install related packages:
```bash
pip install -r requirements.txt
```
3. Install Java to use METEOR evaluation package (for Scan2Cap evaluations).
4. Download [our compiled data](https://huggingface.co/datasets/kmichiru/SVC), and change `SVC_PATH` in `fuyu_utils.py` to your downloaded path.


## Results

## Data
We offer all converted data in this [Huggingface Repo](https://huggingface.co/datasets/kmichiru/SVC), including:
- MV-ScanQA benchmark dataset and TripAlign pre-train datasets and calculated instruction-view pairs for existing 3D vision-language datasets. We offer two versions of TripAlign: one with captions from LLaVA-1.5-7B, one with captions from GPT-4o.
- pre-trained 3D detector from Vote2Cap-DETR [ðŸš§] and its pre-extracted 3D object features.
- pre-trained 3D feature adapter from 1st stage pre-training, and model checkpoints from 2nd stage pre-training, and for MV-ScanQA. [ðŸš§]

### 2D+3D => Text: Caption Generation for TripAlign
We use pre-trained LVLM to generate captions for TripAlign dataset. The captions can be generated by either LLaVA-1.5-7B or and GPT-4o:
```bash
# [TODO] Caption with LLaVA
```
```bash
# [TODO] Caption with GPT-4o
```
We found GPT-4o captions are more accurate and detailed, while LLaVA-1.5-7B captions are more concise. We recommend using GPT-4o captions for better performance.

### 3D+Text => 2D: Extending Existing 3D Vision-Language Datasets with Paired Views
For downstream tasks on existing 3D vision-language datasets, including ScanQA, SQA3D, Scan2Cap (on ScanRefer), and Scan2Cap (on Nr3D), run the following command to select views for instructions:
```bash
# [TODO]
```
These paired views are selected only with the instruction text, without any access to task response annotations. They're used in both pre-training stage and inference stage.

## Training

### Pre-extract 3D Object Features
1. Download pre-trained 3D detector from [Vote2Cap-DETR](ch3cook-fdu/Vote2Cap-DETR), or from our Huggingface Repo.
2. Run the following command to extract 3D object features:
```bash
# [TODO]
```


### TripAlign Pre-training
1. 1st stage pre-train a 3D feature adapter on a subset of TripAlign dataset, with LVLM frozen:
```bash
# [TODO]
```
NOTE: We found 1st stage pre-train makes only small difference on the final performance, so we recommend to skip this stage if you want to save time.
2. 2nd stage pre-train the full model on the full TripAlign dataset, with LoRA:
```bash
# [TODO]
```

### Finetuning on Downstream Tasks
We only found finetuning beneficial for MV-ScanQA and SQA3D, so we provide the finetuning code for these two tasks. For other tasks, we recommend to use the pre-trained model directly.
```bash
# [TODO] MV-ScanQA finetuning
```
```bash
# [TODO] SQA3D finetuning
```


## Acknowledgements
We would like to thank [facebookresearch/votenet](https://github.com/facebookresearch/votenet) and [ch3cook-fdu/Vote2Cap-DETR](https://github.com/ch3cook-fdu/Vote2Cap-DETR) for the 3D object detectors.

## Citation
If you find this codebase useful, please consider citing our work:
```bibtex
```

## License
This code repository and datasets are licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/) license.

Copyright (c) 2025 Wentao Mo.
